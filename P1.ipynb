{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 : Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import math\n",
    "import re\n",
    "\n",
    "# Truthful and deceptive training sets\n",
    "with open('./DATASET/train/truthful.txt') as t, open('./DATASET/train/deceptive.txt') as d:\n",
    "    T_TRAIN = t.read()\n",
    "    D_TRAIN = d.read()\n",
    "    \n",
    "# Truthful and deceptive validation sets\n",
    "with open('./DATASET/validation/truthful.txt') as t, open('./DATASET/validation/deceptive.txt') as d:\n",
    "    T_VAL = t.read()\n",
    "    D_VAL = d.read()\n",
    "\n",
    "# Test set\n",
    "with open('./DATASET/test/test.txt') as t:\n",
    "    TEST = t.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 : Unsmoothed N-Grams\n",
    "### 3.1 : Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 631,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    \"\"\"\n",
    "    Returns a formatted list of the words in the input string [text]\n",
    "    \n",
    "    preprocess adds start characters at the beginning of each review in\n",
    "    [text] and then splits the string on spaces\n",
    "    \n",
    "    text: the input string to be processed\n",
    "    \"\"\"\n",
    "    return add_start_characters(text).split()\n",
    "    \n",
    "def add_start_characters(words):\n",
    "    \"\"\"\n",
    "    Returns the modified string [words] with all newline [\\n] characters replaced with start [<s>] characters\n",
    "    \n",
    "    words: a string\n",
    "    \"\"\"\n",
    "    words = '<s> ' + words\n",
    "    words = words.replace('\\n', ' <s> ')\n",
    "    return words[:-5] if words[-5:] == ' <s> ' else words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 : Unsmoothed Unigram Probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 632,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unigram_corpus(wordlist):\n",
    "    \"\"\"\n",
    "    Returns the unigram corpus given by the input [wordlist]\n",
    "    \n",
    "    get_unigram_corpus counts the number of occurrences of each unique token in [wordlist]\n",
    "    and returns a dictionary where key:value pairings represent unigram:count\n",
    "    \n",
    "    wordlist: a list of words (strings)\n",
    "    \"\"\"\n",
    "    return dict(Counter(wordlist))\n",
    "\n",
    "def get_unigram_prob(unigram, unigram_corpus):\n",
    "    \"\"\"\n",
    "    Returns the probability of a given [unigram] on a given [unigram_corpus]\n",
    "    \n",
    "    get_unigram_prob returns the ratio of the value of [unigram] in the dictionary [unigram_corpus] \n",
    "    (or 1 if it is not present) to the total sum of all values in the dictionary (where the values \n",
    "    are the counts of each unigram in the [unigram_corpus])\n",
    "    \n",
    "    unigram: a unigram (string)\n",
    "    unigram_corpus: a dictionary of unigram:count pairings\n",
    "    \"\"\"\n",
    "    acc = 0\n",
    "    for key in unigram_corpus:\n",
    "        acc += unigram_corpus[key]\n",
    "    return unigram_corpus.get(unigram, 1) / acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 : Unsmoothed Bigram Probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 633,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bigram_corpus(wordlist):\n",
    "    \"\"\"\n",
    "    Returns the bigram corpus given by the input [wordlist]\n",
    "    \n",
    "    get_bigram_corpus creates a dictionary with bigrams from the [wordlist]\n",
    "    as keys and counts the instances of each bigram to assign values (except\n",
    "    for the bigram ('.', '<s>') which represents the start of one review and\n",
    "    the end of another)\n",
    "    \n",
    "    wordlist: a list of words (strings)\n",
    "    \"\"\"\n",
    "    corpus = {}\n",
    "    for i, word in enumerate(wordlist[1:], start=1):\n",
    "        if word != '<s>':\n",
    "            if (wordlist[i-1], word) not in corpus:\n",
    "                corpus[(wordlist[i-1], word)] = 1\n",
    "            else:\n",
    "                corpus[(wordlist[i-1], word)] += 1\n",
    "    return corpus\n",
    "\n",
    "def get_bigram_prob(bigram, bigram_corpus):\n",
    "    \"\"\"\n",
    "    Returns the probability of a given [bigram] on a given [bigram_corpus]\n",
    "    \n",
    "    get_bigram_corpus takes the ratio of the value of [bigram] in the dictionary\n",
    "    [bigram_corpus] (or 1 if it is not present) to the sum of the values of all bigrams\n",
    "    (keys) in the dictionary such that the they have the same first word as [bigram]\n",
    "    \n",
    "    bigram: a bigram (tuple of strings)\n",
    "    bigram_corpus: a dictionary of bigram:count pairings\n",
    "    \"\"\"\n",
    "    acc = 0\n",
    "    for key in bigram_corpus:\n",
    "        if key[0] == bigram[0]:\n",
    "            acc += bigram_corpus[key]\n",
    "    return bigram_corpus.get(bigram, 1) / acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 : Smoothing and Unknown Words\n",
    "### 4.1 : Unknown Word Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 634,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_for_unk_words(wordlist, tokenlist):\n",
    "    \"\"\"\n",
    "    Returns the [wordlist] filtered by the [tokenlist]\n",
    "    \n",
    "    check_for_unk_words takes any element of [wordlist] and replaces it with the\n",
    "    unknown character string [<UNK>] if it does not exist in the [tokenlist]\n",
    "    \n",
    "    wordlist: a list of words (strings)\n",
    "    tokenlist: a list of tokens (strings)\n",
    "    \"\"\"\n",
    "    for i, token in enumerate(wordlist):\n",
    "        if token not in tokenlist:\n",
    "              wordlist[i] = '<UNK>'\n",
    "    return wordlist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 : Smooth Bigram Probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 635,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_smooth_bigram_corpus(tokenlist, bigram_corpus):\n",
    "    \"\"\"\n",
    "    Returns a dataframe object where the columns and rows are labeled with the tokens\n",
    "    in [tokenlist] and the elements are the counts of each bigram (defaulting to 1 to handle\n",
    "    add-one smoothing) in the format (row, column) so that the count for bigram (x, y) is \n",
    "    found with df.loc[x, by]\n",
    "    \n",
    "    get_smooth_bigram_corpus also appends the unknown word character [<UNK>] to handle \n",
    "    unknown words\n",
    "    \n",
    "    tokenlist: a list of tokens (strings)\n",
    "    bigram_corpus: a dictionary of bigram:count pairings\n",
    "    \"\"\"\n",
    "    tokenlist.append('<UNK>')\n",
    "    df = pd.DataFrame(1, index = tokenlist, columns = tokenlist) \n",
    "    for bigram in bigram_corpus:\n",
    "        df.loc[bigram[0], bigram[1]] += bigram_corpus[bigram]\n",
    "    return df\n",
    "\n",
    "def get_smooth_bigram_prob(bigram, smooth_bigram_corpus):\n",
    "    \"\"\"\n",
    "    Returns the probability of a given [bigram] on a given [smooth_bigram_corpus]\n",
    "    \n",
    "    get_smooth_bigram_prob takes the ratio of the value of [bigram] (df.loc(bigram[0], bigram[1]))\n",
    "    in the table [smooth_bigram_corpus] to the sum of all elements in the same row\n",
    "    \n",
    "    bigram: a bigram (tuple of strings)\n",
    "    smooth_bigram_corpus: a dataframe with tokens as row and column names and bigram counts as values\n",
    "    \"\"\"\n",
    "    return smooth_bigram_corpus.loc[bigram[0], bigram[1]] / smooth_bigram_corpus.loc[bigram[0]].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 : Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 636,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGramModel():\n",
    "    \"\"\"\n",
    "    Basic format for n-gram language based model for opinion spam classification\n",
    "    \"\"\"\n",
    "    def __init__(self, *args):\n",
    "        super(NGramModel, self).__init__()\n",
    "\n",
    "    \"Calculate the perplexity of the model on a given corpus\"\n",
    "    def get_perp(self, *args):\n",
    "        return\n",
    "\n",
    "\n",
    "\n",
    "class UnigramModel(NGramModel):\n",
    "    \"\"\"\n",
    "    Unigram language based model for opinion spam classification\n",
    "    \n",
    "    data: a preprocessed list of words (strings)\n",
    "    \"\"\"\n",
    "    def __init__(self, data):\n",
    "        super(UnigramModel, self).__init__()\n",
    "        self.corpus = get_unigram_corpus(data)\n",
    "\n",
    "    \"\"\"\n",
    "    Returns the perplexity of the unigram model on the [test_corpus]\n",
    "    \n",
    "    get_perp exponentiates the normalized sum of the log probabilities of each\n",
    "    token (unigram) in the [test_corpus]\n",
    "    \n",
    "    test_corpus: a preprocessed corpus (list of strings)\n",
    "    \"\"\"\n",
    "    def get_perp(self, test_corpus):\n",
    "        N = len(test_corpus)\n",
    "        acc = 0\n",
    "        for word in test_corpus:\n",
    "            acc -= math.log(get_unigram_prob(word, self.corpus))\n",
    "        return math.exp((1/N) * acc)\n",
    "\n",
    "\n",
    "\n",
    "class BigramModel(NGramModel):\n",
    "    \"\"\"\n",
    "    Bigram language based model for opinion spam classification\n",
    "    \n",
    "    data: a preprocessed list of words (strings)\n",
    "    \"\"\"\n",
    "    def __init__(self, data):\n",
    "        super(BigramModel, self).__init__()\n",
    "        self.tokens = list(get_unigram_corpus(data).keys())\n",
    "        self.corpus = get_bigram_corpus(data)\n",
    "\n",
    "    \"\"\"\n",
    "    Returns the perplexity of the bigram model on the [test_corpus]\n",
    "    \n",
    "    get_perp exponentiates the normalized sum of the log probabilities of each\n",
    "    bigram in the [test_corpus]; for N tokens in the [test_corpus], we create N-1 \n",
    "    bigrams and take the probabilities of those, eventually normalizing by dividing by N-1\n",
    "    \n",
    "    test_corpus: a preprocessed corpus (list of strings)\n",
    "    \"\"\"    \n",
    "    def get_perp(self, test_corpus):\n",
    "        N = len(test_corpus)\n",
    "        acc = 0\n",
    "        for i, word in enumerate(test_corpus):\n",
    "            if i == 0:\n",
    "                continue\n",
    "            bigram = (test_corpus[i-1], word)\n",
    "            acc -= math.log(get_bigram_prob(bigram, self.corpus))\n",
    "        return math.exp(1/(N-1) * acc)\n",
    "\n",
    "\n",
    "\n",
    "class SmoothBigramModel(NGramModel):\n",
    "    \"\"\"\n",
    "    Smooth bigram language based model for opinion spam classification\n",
    "    \n",
    "    data: a preprocessed list of words (strings)\n",
    "    \"\"\"\n",
    "    def __init__(self, data):\n",
    "        super(SmoothBigramModel, self).__init__()\n",
    "        self.tokens = list(get_unigram_corpus(data).keys())\n",
    "        corpus = get_bigram_corpus(data)\n",
    "        self.corpus = get_smooth_bigram_corpus(self.tokens, corpus)\n",
    "\n",
    "    \"\"\"\n",
    "    Returns the perplexity of the smooth bigram model on the [test_corpus]\n",
    "    \n",
    "    get_perp exponentiates the normalized sum of the log probabilities of each\n",
    "    bigram in the [test_corpus]; for N tokens in the [test_corpus], we create N-1 \n",
    "    bigrams and take the probabilities of those, eventually normalizing by dividing by N-1\n",
    "    \n",
    "    test_corpus: a preprocessed corpus (list of strings)\n",
    "    \"\"\"\n",
    "    def get_perp(self, test_corpus):\n",
    "        N = len(test_corpus)\n",
    "        acc = 0\n",
    "        for i, word in enumerate(test_corpus):\n",
    "            if i == 0:\n",
    "                continue\n",
    "            bigram = (test_corpus[i-1], word)\n",
    "            acc -= math.log(get_smooth_bigram_prob(bigram, self.corpus))\n",
    "        return math.exp(1/(N-1) * acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 637,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODELS: a unigram model and a smooth bigram model\n",
    "\n",
    "truthful_unigram_model = UnigramModel(preprocess(T_TRAIN))\n",
    "deceptive_unigram_model = UnigramModel(preprocess(D_TRAIN))\n",
    "\n",
    "truthful_smooth_bigram_model = SmoothBigramModel(preprocess(T_TRAIN))\n",
    "deceptive_smooth_bigram_model = SmoothBigramModel(preprocess(D_TRAIN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 638,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VALIDATION DATA\n",
    "\n",
    "# Truthful validation reviews filtered on truthful training tokens\n",
    "T_VAL_T = check_for_unk_words(preprocess(T_VAL), truthful_smooth_bigram_model.tokens)\n",
    "\n",
    "# Deceptive validation reviews filtered on truthful training tokens\n",
    "D_VAL_T = check_for_unk_words(preprocess(D_VAL), truthful_smooth_bigram_model.tokens)\n",
    "\n",
    "# Truthful validation reviews filtered on truthful deceptive tokens\n",
    "T_VAL_D = check_for_unk_words(preprocess(T_VAL), deceptive_smooth_bigram_model.tokens)\n",
    "\n",
    "# Deceptive validation reviews filtered on truthful deceptive tokens\n",
    "D_VAL_D = check_for_unk_words(preprocess(D_VAL), deceptive_smooth_bigram_model.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 640,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "575.8911106150354\n",
      "507.2203959017036\n",
      "615.8537298038397\n",
      "463.8680901825986\n",
      "1454.340989231253\n",
      "1281.9503153726475\n",
      "1324.6771814849794\n",
      "958.4681090581158\n"
     ]
    }
   ],
   "source": [
    "# PERPLEXITIES\n",
    "\n",
    "print(truthful_unigram_model.get_perp(T_VAL_T))\n",
    "print(truthful_unigram_model.get_perp(D_VAL_T))\n",
    "print(deceptive_unigram_model.get_perp(T_VAL_D))\n",
    "print(deceptive_unigram_model.get_perp(D_VAL_D))\n",
    "\n",
    "print(truthful_smooth_bigram_model.get_perp(T_VAL_T))\n",
    "print(truthful_smooth_bigram_model.get_perp(D_VAL_T))\n",
    "print(deceptive_smooth_bigram_model.get_perp(T_VAL_D))\n",
    "print(deceptive_smooth_bigram_model.get_perp(D_VAL_D))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 643,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATASET PROCESSING TOOLS\n",
    "\n",
    "def separate_reviews(wordlist):\n",
    "    \"\"\"\n",
    "    Returns the wordlist (which should correspond to a list of reviews) separated by \n",
    "    start [<s>] characters, so that each element of the return result is itself a list\n",
    "    of the words of a single review\n",
    "    \n",
    "    wordlist: a list of words (strings)\n",
    "    \"\"\"\n",
    "    start = 0\n",
    "    reviews = []\n",
    "    for i, word in enumerate(wordlist):\n",
    "        if word == '<s>' and i != 0:\n",
    "            reviews.append(wordlist[start:i])\n",
    "            start = i+1\n",
    "    return reviews\n",
    "\n",
    "def separate_and_label_reviews(wordlist, label):\n",
    "    \"\"\"\n",
    "    Returns a dictionary mapping the wordlist (which should correspond to a list of reviews) separated by \n",
    "    start [<s>] characters, so that each element of the return result is itself a list\n",
    "    of the words of a single review\n",
    "    \n",
    "    wordlist: a list of words (strings)\n",
    "    \"\"\"\n",
    "    sep_reviews = separate_reviews(wordlist)\n",
    "    reviews = {}\n",
    "    for review in sep_reviews:\n",
    "        reviews[tuple(review)] = label\n",
    "    return reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 644,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOKENS is an intersecting set of all tokens from the truthful training data and the deceptive training data\n",
    "TOKENS = list(set(truthful_smooth_bigram_model.tokens) & set(deceptive_smooth_bigram_model.tokens))\n",
    "\n",
    "# VAL_REVIEWS_T is a list of words from the truthful validation set filtered on TOKENS \n",
    "VAL_REVIEWS_T = check_for_unk_words(preprocess(T_VAL), TOKENS)\n",
    "\n",
    "# VAL_REVIEWS_D is a list of words from the deceptive validation set filtered on TOKENS \n",
    "VAL_REVIEWS_D = check_for_unk_words(preprocess(D_VAL), TOKENS)\n",
    "\n",
    "# VAL_REVIEWS is a dictionary where the keys are wordlists of the validation (truthful and deceptive) reviews\n",
    "# and the values are the classification labels, with 0 representing truthful and 1 deceptive,\n",
    "# and all possible unknown words have been filtered out\n",
    "VAL_REVIEWS = {**separate_and_label_reviews(VAL_REVIEWS_T, 0), **separate_and_label_reviews(VAL_REVIEWS_D, 1)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 645,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(review, tmodel, dmodel):\n",
    "    \"\"\"\n",
    "    Returns 0 if the models predict that the review is truthful, 1 otherwise\n",
    "    \n",
    "    classify compares the relative perplexities of truthfully-trained and deceptively-trained\n",
    "    models on the review and chooses the classification based on which model computes a smaller\n",
    "    perplexity on the corpus in question\n",
    "    \n",
    "    review: a preprocessed list of words (strings)\n",
    "    tmodel: a language based model trained on truthful reviews\n",
    "    dmodel: a language based model trained on deceptive reviews\n",
    "    \"\"\"\n",
    "    return 0 if tmodel.get_perp(review) < dmodel.get_perp(review) else 1\n",
    "\n",
    "def validate(reviews, tmodel, dmodel):\n",
    "    \"\"\"\n",
    "    Returns the accuracy of a language based model on the [reviews]\n",
    "    \n",
    "    validate takes the ratio of the number of correct classification predictions\n",
    "    of the model on the [reviews] and the total number of reviews to compute\n",
    "    the accuracy of the model\n",
    "    \n",
    "    reviews: a dictionary mapping wordlists (reviews) to labels (0, 1)\n",
    "    tmodel: a language based model trained on truthful reviews\n",
    "    dmodel: a language based model trained on deceptive reviews\n",
    "    \"\"\"\n",
    "    acc = 0\n",
    "    for review in reviews:\n",
    "        if classify(review, tmodel, dmodel) == reviews[review]:\n",
    "            acc += 1\n",
    "    return acc / len(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 646,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8740157480314961"
      ]
     },
     "execution_count": 646,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute the accuracy of the smooth bigram model on the validation set\n",
    "print(validate(VAL_REVIEWS, truthful_smooth_bigram_model, deceptive_smooth_bigram_model))\n",
    "\n",
    "# Compute the accuracy of the unigram model on the validation set\n",
    "print(validate(VAL_REVIEWS, truthful_unigram_model, deceptive_unigram_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 : Kaggle Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 647,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(reviews, tmodel, dmodel):\n",
    "    \"\"\"\n",
    "    Returns a dataframe with two columns, Ids and Predictions, where\n",
    "    Ids are the indices of the order of the test reviews, and Predictions\n",
    "    are the predicted classifications of the model (0, 1)\n",
    "    \n",
    "    reviews: a dictionary mapping wordlists (reviews) to labels (0, 1)\n",
    "    tmodel: a language based model trained on truthful reviews\n",
    "    dmodel: a language based model trained on deceptive reviews\n",
    "    \"\"\"\n",
    "    ids = [i for i in range(len(reviews))]\n",
    "    preds = []\n",
    "    for review in reviews:\n",
    "        preds.append(classify(review, tmodel, dmodel))\n",
    "    df = pd.DataFrame({'Id': ids, 'Prediction': preds}, columns = ['Id', 'Prediction'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 650,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>315</td>\n",
       "      <td>315</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>316</td>\n",
       "      <td>316</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>317</td>\n",
       "      <td>317</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>318</td>\n",
       "      <td>318</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>319</td>\n",
       "      <td>319</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>320 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Id  Prediction\n",
       "0      0           0\n",
       "1      1           1\n",
       "2      2           1\n",
       "3      3           1\n",
       "4      4           1\n",
       "..   ...         ...\n",
       "315  315           1\n",
       "316  316           1\n",
       "317  317           1\n",
       "318  318           1\n",
       "319  319           1\n",
       "\n",
       "[320 rows x 2 columns]"
      ]
     },
     "execution_count": 650,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TEST_REVIEWS is a list of words from the test set filtered on TOKENS\n",
    "# and then partitioned into individual reviews\n",
    "TEST_REVIEWS = check_for_unk_words(preprocess(TEST), TOKENS)\n",
    "TEST_REVIEWS = separate_reviews(TEST_REVIEWS)\n",
    "TEST_REVIEWS.append([])\n",
    "\n",
    "# df is a dataframe of the smooth bigram model's predictions for the reviews in the test set\n",
    "df = test(TEST_REVIEWS, truthful_smooth_bigram_model, deceptive_smooth_bigram_model)\n",
    "df.to_csv('Smooth-Bigram-LM.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Toy example for verifying correctness of smooth bigram probabilities\n",
    "\n",
    "# toy_train = preprocess('I like to run and I hate to have to do homework')\n",
    "# toy_model = SmoothBigramModel(toy_train)\n",
    "# print(toy_model.corpus)\n",
    "# toy_val = check_for_unk_words(preprocess('I like homework'), toy_model.tokens)\n",
    "# print(toy_model.get_perp(toy_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 : Feature-based Naive Bayes for Opinion Spam Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 1: Unigram/Bag of Words Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class bag_of_words_nb_classifier():\n",
    "    def __init__(self, training_truthful_text, training_deceptive_text):\n",
    "        self.truthful_text = self.add_start_characters(training_truthful_text)\n",
    "        self.deceptive_text = self.add_start_characters(training_deceptive_text)\n",
    "        \n",
    "        self.truthful_reviews_Counter = Counter(self.truthful_text.split())\n",
    "        self.deceptive_reviews_Counter = Counter(self.deceptive_text.split())\n",
    "        \n",
    "        self.truthful_total_words = sum(self.truthful_reviews_Counter.values())\n",
    "        self.deceptive_total_words = sum(self.deceptive_reviews_Counter.values())\n",
    "        \n",
    "        self.truthful_reviews_len = len(self.truthful_text.split('\\n'))\n",
    "        self.deceptive_reviews_len = len(self.truthful_text.split('\\n'))\n",
    "\n",
    "        self.both_reviews_Counter = self.truthful_reviews_Counter + self.deceptive_reviews_Counter\n",
    "        self.vocabulary_size = len(self.both_reviews_Counter.keys())\n",
    "        self.word_pattern = re.compile(\"(\\w+|<s> |[,.!;])\")\n",
    "        \n",
    "        self.k = 0.2\n",
    "\n",
    "    def add_start_characters(self, words):\n",
    "        words = '<s> ' + words\n",
    "        words = words.replace('\\n', ' <s> ')\n",
    "        return words[:-5]\n",
    "        \n",
    "    def smoothed_word_log_prob(self, word, counter, total):\n",
    "        return math.log((counter[word] + self.k) / (total + (self.vocabulary_size*self.k)))\n",
    "    \n",
    "    \n",
    "    def smoothed_review_log_prob(self, review, counter, total):\n",
    "        log_prob = 0.0\n",
    "        for word in self.word_pattern.findall(review):\n",
    "            log_prob += self.smoothed_word_log_prob(word, counter, total)\n",
    "        return log_prob\n",
    "\n",
    "    \n",
    "    def classify_review(self, review):\n",
    "        review = '<s> ' + review\n",
    "        truthful_prob = self.smoothed_review_log_prob(review,\n",
    "                            self.truthful_reviews_Counter, self.truthful_total_words)\n",
    "        deceptive_prob = self.smoothed_review_log_prob(review,\n",
    "                                self.deceptive_reviews_Counter, self.deceptive_total_words)\n",
    "\n",
    "        # get ratio between the two (since this training set has the same number of reviews, this code is optional)\n",
    "        truthful_prob = truthful_prob + \\\n",
    "            math.log(self.truthful_reviews_len/(self.truthful_reviews_len + self.deceptive_reviews_len))\n",
    "        deceptive_prob = deceptive_prob + \\\n",
    "            math.log(self.deceptive_reviews_len /(self.truthful_reviews_len + self.deceptive_reviews_len))\n",
    "\n",
    "        return 0 if truthful_prob >= deceptive_prob else 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./DATASET/train/truthful.txt') as t, open('./DATASET/train/deceptive.txt') as d:\n",
    "    truthful = t.read()\n",
    "    deceptive = d.read()\n",
    "nb = bag_of_words_nb_classifier(truthful, deceptive)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./DATASET/validation/truthful.txt') as t, open('./DATASET/validation/deceptive.txt') as d:\n",
    "    truthful_validation_text = t.read()\n",
    "    deceptive_validation_text = d.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 120, 1: 9})\n",
      "Accuracy rate: 0.9302325581395349\n"
     ]
    }
   ],
   "source": [
    "truthful_validation_classifications = \\\n",
    "    [nb.classify_review(review) for review in truthful_validation_text.split('\\n')]\n",
    "truthful_validation_accuracy_counts = Counter(truthful_validation_classifications)\n",
    "print(truthful_validation_accuracy_counts)\n",
    "print('Accuracy rate:', \n",
    "      truthful_validation_accuracy_counts[0]/sum(truthful_validation_accuracy_counts.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({1: 121, 0: 8})\n",
      "Accuracy rate: 0.937984496124031\n"
     ]
    }
   ],
   "source": [
    "deceptive_validation_classifications = \\\n",
    "    [nb.classify_review(review) for review in deceptive_validation_text.split('\\n')]\n",
    "deceptive_validation_accuracy_counts = Counter(deceptive_validation_classifications)\n",
    "print(deceptive_validation_accuracy_counts)\n",
    "print('Accuracy rate:', \n",
    "      deceptive_validation_accuracy_counts[1]/sum(deceptive_validation_accuracy_counts.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./DATASET/test/test.txt') as t:\n",
    "    test_text = t.read()\n",
    "\n",
    "    deceptive_validation_classifications = \\\n",
    "    test_results=[nb.classify_review(review) for review in test_text.split('\\n')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = test_results[:-1]\n",
    "test_ids = [id_ for id_ in range(0, len(test_text.split('\\n')) - 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  Prediction\n",
       "0   0           0\n",
       "1   1           0\n",
       "2   2           1\n",
       "3   3           1\n",
       "4   4           0\n",
       "5   5           0\n",
       "6   6           1\n",
       "7   7           1\n",
       "8   8           1\n",
       "9   9           0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({'Id' : test_ids,'Prediction':test_results}, columns=['Id', 'Prediction'])\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('naive_bayes_unigram.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 2: Bigram Naive Bayes Classifier\n",
    "* Built on top of the NLTK Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.classify.util\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class bigram_nb_classifier():\n",
    "    '''\n",
    "    A bigram naive bayes classifier built on top of the NLTK library.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    def __init__(self, truthful_train_txt, deceptive_train_txt):\n",
    "        '''\n",
    "        Initialize with the raw text of the reviews from the text files, delimited by newlines.\n",
    "        \n",
    "        truthful_train_txt: One string containing all truthful reviews to be trained, delimited by newlines.\n",
    "        deceptive_train_txt: One string all truthful reviews to be trained, delimited by newlines.\n",
    "        '''\n",
    "        truthful_data = [(self.create_ngram_features(review.split()), 'truthful') \\\n",
    "                         for review in self.preprocess(truthful_train_txt)]\n",
    "        deceptive_data = [(self.create_ngram_features(review.split()), 'deceptive') \\\n",
    "                          for review in self.preprocess(deceptive_train_txt)]\n",
    "        self.training_data = deceptive_data + truthful_data\n",
    "        self.classifier = NaiveBayesClassifier.train(self.training_data)\n",
    "        self.latest_accuracy = -1\n",
    "        \n",
    "        stoplist = set(stopwords.words(\"english\"))\n",
    "    \n",
    "    # Method from \n",
    "    # https://stackoverflow.com/questions/48003907/how-to-train-naive-bayes-classifier-for-n-gram-movie-reviews\n",
    "    def create_ngram_features(self, words, n=2):\n",
    "        '''\n",
    "        Will create a dictionary of bigrams in the form {(word1, word2), True} for input into the NLTK \n",
    "        classifier. \n",
    "        \n",
    "        words: words of 1 review to be converted into Bigrams.\n",
    "        n: n value in n-grams.\n",
    "        '''\n",
    "        ngram_vocab = ngrams(words, n)\n",
    "        my_dict = dict([(ng, True) for ng in ngram_vocab])\n",
    "        return my_dict\n",
    "    \n",
    "    def compute_accuracy(self, truthful_val_txt, deceptive_val_txt):\n",
    "        '''\n",
    "        Computes the accuracy against the validation set.\n",
    "        \n",
    "        truthful_val_txt: One string containing all truthful reviews to be validated, \n",
    "            delimited by newlines.\n",
    "        deceptive_val_txt: One string containing all deceptive reviews to be validated, \n",
    "            delimited by newlines.\n",
    "        '''\n",
    "        truthful_data_v = [(self.create_ngram_features(review.split()), 'truthful') \\\n",
    "                           for review in self.preprocess(truthful_val_txt)]\n",
    "        deceptive_data_v = [(self.create_ngram_features(review.split()), 'deceptive') \\\n",
    "                            for review in self.preprocess(deceptive_val_txt)]\n",
    "        validation_data = truthful_data_v + deceptive_data_v \n",
    "        self.latest_accuracy = nltk.classify.util.accuracy(self.classifier, validation_data)\n",
    "        return self.latest_accuracy\n",
    "    \n",
    "    def add_start_character(self, review):\n",
    "        '''\n",
    "        Adds a start token (<s>) to the beginning of the string.\n",
    "        \n",
    "        review: Review to add the token to the start of.\n",
    "        '''\n",
    "        return '<s> ' + review\n",
    "\n",
    "    def preprocess(self, text):\n",
    "        '''\n",
    "        Splits reviews from the review list into individual reviews and adds a start character to \n",
    "        each one. \n",
    "        \n",
    "        text: Raw text from training text as a string.\n",
    "        '''\n",
    "        review_list = text.split('\\n')[:-1]\n",
    "        processed_review_list = [self.add_start_character(review) for review in review_list]\n",
    "        return processed_review_list\n",
    "    \n",
    "    def classify_review(self, review):\n",
    "        '''\n",
    "        Classifies a review as either truthful or deceptive.\n",
    "        \n",
    "        review: Review as a string to be classified.\n",
    "        '''\n",
    "        review = self.add_start_character(review)\n",
    "        return self.classifier.classify(self.create_ngram_features(review.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./DATASET/train/truthful.txt') as t, open('./DATASET/train/deceptive.txt') as d:\n",
    "    truthful_train_txt = t.read()\n",
    "    deceptive_train_txt = d.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb = bigram_nb_classifier(truthful_train_txt, deceptive_train_txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./DATASET/validation/truthful.txt') as t, open('./DATASET/validation/deceptive.txt') as d:\n",
    "    truthful_val_txt = t.read()\n",
    "    deceptive_val_txt = d.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9828125"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bnb.compute_accuracy(truthful_val_txt, deceptive_train_txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./DATASET/test/test.txt') as t:\n",
    "    test_txt = t.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "truthful\n",
      "truthful\n",
      "deceptive\n",
      "deceptive\n",
      "truthful\n",
      "truthful\n",
      "deceptive\n",
      "deceptive\n",
      "deceptive\n",
      "truthful\n",
      "deceptive\n",
      "deceptive\n",
      "deceptive\n",
      "truthful\n",
      "deceptive\n",
      "deceptive\n",
      "deceptive\n",
      "truthful\n",
      "deceptive\n",
      "truthful\n",
      "truthful\n",
      "deceptive\n",
      "deceptive\n",
      "deceptive\n",
      "deceptive\n",
      "deceptive\n",
      "deceptive\n",
      "truthful\n",
      "deceptive\n",
      "truthful\n",
      "deceptive\n",
      "truthful\n",
      "truthful\n",
      "truthful\n",
      "deceptive\n",
      "truthful\n",
      "deceptive\n",
      "truthful\n",
      "deceptive\n",
      "deceptive\n",
      "deceptive\n",
      "deceptive\n",
      "deceptive\n",
      "deceptive\n",
      "deceptive\n",
      "deceptive\n",
      "truthful\n",
      "deceptive\n",
      "deceptive\n",
      "deceptive\n",
      "truthful\n",
      "deceptive\n",
      "truthful\n",
      "deceptive\n",
      "deceptive\n",
      "deceptive\n",
      "truthful\n",
      "deceptive\n",
      "truthful\n",
      "deceptive\n",
      "deceptive\n",
      "deceptive\n",
      "truthful\n",
      "deceptive\n",
      "truthful\n",
      "deceptive\n",
      "truthful\n",
      "truthful\n",
      "truthful\n",
      "deceptive\n",
      "deceptive\n",
      "truthful\n",
      "truthful\n",
      "truthful\n",
      "truthful\n",
      "deceptive\n",
      "deceptive\n",
      "deceptive\n",
      "deceptive\n",
      "deceptive\n",
      "truthful\n",
      "truthful\n",
      "truthful\n",
      "truthful\n",
      "truthful\n",
      "truthful\n",
      "truthful\n",
      "truthful\n",
      "truthful\n",
      "truthful\n",
      "deceptive\n",
      "truthful\n",
      "truthful\n",
      "truthful\n",
      "deceptive\n",
      "deceptive\n",
      "deceptive\n",
      "deceptive\n",
      "truthful\n",
      "truthful\n",
      "truthful\n",
      "truthful\n",
      "truthful\n",
      "deceptive\n",
      "truthful\n",
      "truthful\n",
      "truthful\n",
      "deceptive\n",
      "deceptive\n",
      "deceptive\n",
      "truthful\n",
      "deceptive\n",
      "truthful\n",
      "deceptive\n",
      "deceptive\n",
      "truthful\n",
      "truthful\n",
      "deceptive\n",
      "deceptive\n",
      "truthful\n",
      "truthful\n",
      "truthful\n",
      "deceptive\n",
      "truthful\n",
      "truthful\n",
      "deceptive\n",
      "truthful\n",
      "deceptive\n",
      "truthful\n",
      "deceptive\n",
      "truthful\n",
      "truthful\n",
      "deceptive\n",
      "deceptive\n",
      "truthful\n",
      "truthful\n",
      "truthful\n",
      "deceptive\n",
      "truthful\n",
      "deceptive\n",
      "deceptive\n",
      "deceptive\n",
      "truthful\n",
      "truthful\n",
      "deceptive\n",
      "deceptive\n",
      "truthful\n",
      "deceptive\n",
      "deceptive\n",
      "truthful\n",
      "deceptive\n",
      "deceptive\n",
      "truthful\n",
      "deceptive\n",
      "deceptive\n",
      "deceptive\n",
      "deceptive\n",
      "deceptive\n",
      "deceptive\n",
      "deceptive\n",
      "truthful\n",
      "truthful\n",
      "deceptive\n",
      "truthful\n",
      "truthful\n",
      "deceptive\n",
      "deceptive\n",
      "deceptive\n",
      "deceptive\n",
      "truthful\n",
      "deceptive\n",
      "deceptive\n",
      "truthful\n",
      "truthful\n",
      "truthful\n",
      "deceptive\n",
      "deceptive\n",
      "truthful\n",
      "deceptive\n",
      "truthful\n",
      "deceptive\n",
      "deceptive\n",
      "truthful\n",
      "deceptive\n",
      "deceptive\n",
      "deceptive\n",
      "truthful\n",
      "truthful\n",
      "deceptive\n",
      "truthful\n",
      "deceptive\n",
      "deceptive\n",
      "deceptive\n",
      "deceptive\n",
      "deceptive\n",
      "truthful\n",
      "truthful\n",
      "truthful\n",
      "deceptive\n",
      "deceptive\n",
      "deceptive\n",
      "deceptive\n",
      "deceptive\n",
      "deceptive\n",
      "deceptive\n",
      "deceptive\n",
      "truthful\n",
      "truthful\n",
      "deceptive\n",
      "truthful\n",
      "truthful\n",
      "deceptive\n",
      "truthful\n",
      "truthful\n",
      "deceptive\n",
      "deceptive\n",
      "truthful\n",
      "deceptive\n",
      "truthful\n",
      "deceptive\n",
      "truthful\n",
      "truthful\n",
      "truthful\n",
      "truthful\n",
      "truthful\n",
      "truthful\n",
      "truthful\n",
      "deceptive\n",
      "truthful\n",
      "deceptive\n",
      "deceptive\n",
      "deceptive\n",
      "truthful\n",
      "truthful\n",
      "truthful\n",
      "truthful\n",
      "deceptive\n",
      "deceptive\n",
      "truthful\n",
      "truthful\n",
      "deceptive\n",
      "truthful\n",
      "truthful\n",
      "deceptive\n",
      "truthful\n",
      "deceptive\n",
      "deceptive\n",
      "deceptive\n",
      "deceptive\n",
      "deceptive\n",
      "truthful\n",
      "deceptive\n",
      "deceptive\n",
      "truthful\n",
      "truthful\n",
      "deceptive\n",
      "deceptive\n",
      "truthful\n",
      "truthful\n",
      "truthful\n",
      "deceptive\n",
      "truthful\n",
      "deceptive\n",
      "truthful\n",
      "deceptive\n",
      "deceptive\n",
      "deceptive\n",
      "truthful\n",
      "truthful\n",
      "truthful\n",
      "deceptive\n",
      "deceptive\n",
      "truthful\n",
      "truthful\n",
      "deceptive\n",
      "deceptive\n",
      "deceptive\n",
      "deceptive\n",
      "truthful\n",
      "truthful\n",
      "truthful\n",
      "truthful\n",
      "deceptive\n",
      "deceptive\n",
      "truthful\n",
      "deceptive\n",
      "truthful\n",
      "truthful\n",
      "deceptive\n",
      "truthful\n",
      "truthful\n",
      "truthful\n",
      "deceptive\n",
      "truthful\n",
      "truthful\n",
      "deceptive\n",
      "truthful\n",
      "truthful\n",
      "truthful\n",
      "truthful\n",
      "deceptive\n",
      "deceptive\n",
      "deceptive\n",
      "deceptive\n",
      "truthful\n",
      "truthful\n",
      "truthful\n",
      "deceptive\n",
      "deceptive\n",
      "deceptive\n",
      "deceptive\n",
      "truthful\n",
      "deceptive\n",
      "truthful\n",
      "deceptive\n",
      "deceptive\n",
      "deceptive\n",
      "truthful\n",
      "deceptive\n",
      "deceptive\n"
     ]
    }
   ],
   "source": [
    "for review in test_txt.split('\\n')[:-1]:\n",
    "    print(bnb.classify_review(review))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
