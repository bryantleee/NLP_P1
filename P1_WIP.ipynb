{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "with open('./DATASET/train/truthful.txt') as t, open('./DATASET/train/deceptive.txt') as d:\n",
    "    T_Train = t.read()\n",
    "    D_Train = d.read()\n",
    "    \n",
    "with open('./DATASET/validation/truthful.txt') as t, open('./DATASET/validation/deceptive.txt') as d:\n",
    "    T_Val = t.read()\n",
    "    D_Val = d.read()\n",
    "    \n",
    "with open('./DATASET/test/test.txt') as t:\n",
    "    Test = t.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    return add_start_characters(text.split())\n",
    "    \n",
    "def add_start_characters(words):\n",
    "    words = '<s> ' + words\n",
    "    words = words.replace('\\n', ' <s> ')\n",
    "    return words[:-5]\n",
    "\n",
    "def check_for_unk_words(word_list, corpus):\n",
    "    # replace all unknown words with <UNK> token\n",
    "    for i, word in enumerate(word_list):\n",
    "        if word not in corpus:\n",
    "              word_list[i] = '<UNK>'\n",
    "    return word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unigram_prob(corpus, unigram_key):\n",
    "    total_words = 0\n",
    "    for key in corpus:\n",
    "        total_words += corpus[key]\n",
    "    return corpus.get(unigram_key, 0)/total_words\n",
    "\n",
    "def get_bigram_counts(word_list):\n",
    "    corpus = {}\n",
    "    for i, word in enumerate(word_list[1:], start=1):\n",
    "        if word != '<s>':\n",
    "            if (word_list[i-1], word) not in corpus:\n",
    "                corpus[(word_list[i-1], word)] = 1\n",
    "            else:\n",
    "                corpus[(word_list[i-1], word)] += 1\n",
    "    return corpus\n",
    "\n",
    "def get_smoothed_bigram_corpus(token_list, bigrams):\n",
    "    df = pd.DataFrame(1, index = token_list, columns = token_list) \n",
    "    for bigram in bigrams:\n",
    "        df.loc[bigram[0], bigram[1]] += bigrams[bigram]\n",
    "    return df\n",
    "\n",
    "def get_smoothed_bigram_prob(bigram, smoothed_bigram_corpus):\n",
    "    return smoothed_bigram_corpus.loc[bigram[0], bigram[1]]/df.loc[bigram[0]].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGramModel():\n",
    "    def __init__(self, *args):\n",
    "        super(Model, self).__init__()\n",
    "    \n",
    "    def get_probability(self, *args):\n",
    "        return\n",
    "        \n",
    "        \n",
    "class UnigramModel(NGramModel):\n",
    "    # assumes dataset is a cleaned, in-order list of the words in the desired input string \n",
    "    # with unknown words NOT yet handled !!!\n",
    "    def __init__(self, data):\n",
    "        super(UnigramModel, self).__init__()\n",
    "        self.data = check_for_unk_words(data, self.model)\n",
    "        self.model = get_n_gram_model(self.data, 1)\n",
    "\n",
    "    def get_probability(self, sequence):\n",
    "        product = 1\n",
    "        for word in sequence:\n",
    "            product = product * get_unigram_prob(model, word)\n",
    "        return product\n",
    "    \n",
    "    \n",
    "    \n",
    "class SmoothBigramModel(NGramModel):\n",
    "    # assumes dataset is a cleaned, in-order list of the words in the desired input string \n",
    "    # with unknown words NOT yet handled !!!\n",
    "    def __init__(self, data):\n",
    "        super(SmoothBigramModel, self).__init__()\n",
    "        self.data = check_for_unk_words(data, self.tokens)\n",
    "        self.tokens = get_n_gram_model(self.data, 1)\n",
    "        self.model = get_n_gram_model(self.data, 2)\n",
    "        self.table = get_smoothed_bigram_corpus(self.tokens, self.model)\n",
    "\n",
    "    def get_probability(self, sequence):\n",
    "        bigrams = get_bigram_counts(sequence)\n",
    "        product = 1\n",
    "        for bigram in bigrams:\n",
    "            product = product * get_smoothed_bigram_prob(bigram, self.table)\n",
    "        return product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_n_gram_model(wordlist, n):\n",
    "    # wordlist is an in-order list of the words on which to build the model\n",
    "    # n determines whether to use unigram or bigram estimation (n must be 1 or 2)\n",
    "    n_gram_model = {}\n",
    "    for i, word in enumerate(wordlist):\n",
    "        if i + n > len(wordlist) - 1:\n",
    "            break\n",
    "        key = tuple([wordlist[i+x] for x in range(n)])\n",
    "        if n == 1:\n",
    "            # by convention, we now use strings as keys for unigram models and tuples for bigram models\n",
    "            key = word\n",
    "        if key not in n_gram_model:\n",
    "            n_gram_model[key] = 1\n",
    "        else:\n",
    "            n_gram_model[key] += 1 \n",
    "    return n_gram_model\n",
    "\n",
    "def get_n_gram_prob(sequence, model):\n",
    "    # model is a dictionary representing the n-grams and counts for some dataset\n",
    "    \n",
    "    # this just gets n implied from the model\n",
    "    for key in model:\n",
    "        if isinstance(key, str):\n",
    "            n = 1\n",
    "        else:\n",
    "            n = len(key)\n",
    "        break\n",
    "        \n",
    "    if n == 1:\n",
    "        # case if model uses unigram estimation\n",
    "        result = 1\n",
    "        for word in sequence:\n",
    "            result = result * get_unigram_prob(model, word)\n",
    "        return result\n",
    "    elif n == 2:\n",
    "        # case if model uses bigram estimation\n",
    "        bigrams = get_bigram_counts(sequence)\n",
    "        result = 1\n",
    "        for bigram in bigrams:\n",
    "            result = result * get_bigram_prob(model, bigram)\n",
    "        return result\n",
    "    else:\n",
    "        # else cannot compute\n",
    "        return 0\n",
    "    \n",
    "def compute_perplexity(test_corpus, model):\n",
    "    # test_corpus: the corpus on which to compute the model's perplexity\n",
    "    # test_corpus is a (unigram-count) dictionary of key:value pairiings where keys are all unique tokens \n",
    "    # for the desired input and values are their counts\n",
    "    N = len(test_corpus)\n",
    "    acc = 0\n",
    "    for token in test_corpus:\n",
    "        acc -= math.log(get_n_gram_prob(n_gram, model))\n",
    "    return math.e ** ((1/N) * acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
