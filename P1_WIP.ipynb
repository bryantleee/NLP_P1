{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 : Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "with open('./DATASET/train/truthful.txt') as t, open('./DATASET/train/deceptive.txt') as d:\n",
    "    T_TRAIN = t.read()\n",
    "    D_TRAIN = d.read()\n",
    "    \n",
    "with open('./DATASET/validation/truthful.txt') as t, open('./DATASET/validation/deceptive.txt') as d:\n",
    "    T_VAL = t.read()\n",
    "    D_VAL = d.read()\n",
    "\n",
    "with open('./DATASET/test/test.txt') as t:\n",
    "    TEST = t.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 : Unsmoothed N-Grams\n",
    "### 3.1 : Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    return add_start_characters(text).split()\n",
    "    \n",
    "def add_start_characters(words):\n",
    "    words = '<s> ' + words\n",
    "    words = words.replace('\\n', ' <s> ')\n",
    "    return words[:-5] if words[-5:] == ' <s> ' else words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 : Unsmoothed Unigram Probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unigram_corpus(wordlist):\n",
    "    return dict(Counter(wordlist))\n",
    "\n",
    "def get_unigram_prob(unigram, unigram_corpus):\n",
    "    acc = 0\n",
    "    for key in unigram_corpus:\n",
    "        acc += unigram_corpus[key]\n",
    "    # I changed the default value to 1 from 0 to reflect smoothing\n",
    "    return unigram_corpus.get(unigram, 1)/acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 : Unsmoothed Bigram Probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bigram_corpus(wordlist):\n",
    "    corpus = {}\n",
    "    for i, word in enumerate(wordlist[1:], start=1):\n",
    "        if word != '<s>':\n",
    "            if (wordlist[i-1], word) not in corpus:\n",
    "                corpus[(wordlist[i-1], word)] = 1\n",
    "            else:\n",
    "                corpus[(wordlist[i-1], word)] += 1\n",
    "    return corpus\n",
    "\n",
    "def get_bigram_prob(bigram, bigram_corpus):\n",
    "    acc = 0\n",
    "    for key in bigram_corpus:\n",
    "        if key[0] == bigram[0]:\n",
    "            acc += bigram_corpus[key]\n",
    "    return bigram_corpus[bigram] / acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 : Smoothing and Unknown Words\n",
    "### 4.1 : Unknown Word Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_for_unk_words(wordlist, tokenlist):\n",
    "    # replace all unknown words with <UNK> token\n",
    "    for i, token in enumerate(wordlist):\n",
    "        if token not in tokenlist:\n",
    "              wordlist[i] = '<UNK>'\n",
    "    return wordlist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 : Smooth Bigram Probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [],
   "source": [
    "# added <UNK> as a token when creating the smooth bigram corpus \n",
    "def get_smooth_bigram_corpus(tokenlist, bigram_corpus):\n",
    "    tokenlist.append('<UNK>')\n",
    "    df = pd.DataFrame(1, index = tokenlist, columns = tokenlist) \n",
    "    for bigram in bigram_corpus:\n",
    "        df.loc[bigram[0], bigram[1]] += bigram_corpus[bigram]\n",
    "    return df\n",
    "\n",
    "def get_smooth_bigram_prob(bigram, smooth_bigram_corpus):\n",
    "    return smooth_bigram_corpus.loc[bigram[0], bigram[1]]/smooth_bigram_corpus.loc[bigram[0]].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 : Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGramModel():\n",
    "    def __init__(self, *args):\n",
    "        super(NGramModel, self).__init__()\n",
    "    \n",
    "    def get_perp(self, *args):\n",
    "        return\n",
    "        \n",
    "    \n",
    "        \n",
    "class UnigramModel(NGramModel):\n",
    "    # assumes data is preprocessed list of words (strings)   \n",
    "    def __init__(self, data):\n",
    "        super(UnigramModel, self).__init__()\n",
    "        self.corpus = get_unigram_corpus(data)\n",
    "\n",
    "    # assumes test_corpus is a preprocessed list of words (strings)\n",
    "    def get_perp(self, test_corpus):\n",
    "        N = len(test_corpus)\n",
    "        acc = 0\n",
    "        for word in test_corpus:\n",
    "            acc -= math.log(get_unigram_prob(word, self.corpus))\n",
    "        return math.exp((1/N) * acc)\n",
    "\n",
    "    \n",
    "    \n",
    "class BigramModel(NGramModel):\n",
    "    # assumes data is preprocessed list of words (strings)\n",
    "    def __init__(self, data):\n",
    "        super(BigramModel, self).__init__()\n",
    "        self.corpus = get_bigram_corpus(data)\n",
    "    \n",
    "    # assumes test_corpus is a preprocessed list of words (strings)\n",
    "    def get_perp(self, test_corpus):\n",
    "        N = len(test_corpus)\n",
    "        acc = 0\n",
    "        for i, word in enumerate(test_corpus):\n",
    "            if i == 0:\n",
    "                # we divide by N-1 because we don't compute the probability of the first term in the corpus\n",
    "                continue\n",
    "            bigram = (test_corpus[i-1], word)\n",
    "            acc -= math.log(get_bigram_prob(bigram, self.corpus))\n",
    "        return math.exp((1/(N-1)) * acc)\n",
    "    \n",
    "    \n",
    "    \n",
    "class SmoothBigramModel(NGramModel):\n",
    "    # assumes data is preprocessed list of words (strings)\n",
    "    def __init__(self, data):\n",
    "        super(SmoothBigramModel, self).__init__()\n",
    "        self.tokens = list(get_unigram_corpus(data).keys())\n",
    "        corpus = get_bigram_corpus(data)\n",
    "        self.corpus = get_smooth_bigram_corpus(self.tokens, corpus)\n",
    "    \n",
    "    # assumes test_corpus is a preprocessed list of words (strings)\n",
    "    def get_perp(self, test_corpus):\n",
    "        N = len(test_corpus)\n",
    "        acc = 0\n",
    "        for i, word in enumerate(test_corpus):\n",
    "            if i == 0:\n",
    "                # we divide by N-1 because we don't compute the probability of the first term in the corpus\n",
    "                continue\n",
    "            bigram = (test_corpus[i-1], word)\n",
    "            acc -= math.log(get_smooth_bigram_prob(bigram, self.corpus))\n",
    "        return math.exp((1/(N-1)) * acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [],
   "source": [
    "truthful_unigram_model = UnigramModel(preprocess(T_TRAIN))\n",
    "deceptive_unigram_model = UnigramModel(preprocess(D_TRAIN))\n",
    "\n",
    "truthful_smooth_bigram_model = SmoothBigramModel(preprocess(T_TRAIN))\n",
    "deceptive_smooth_bigram_model = SmoothBigramModel(preprocess(D_TRAIN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for computing perplexity of both validation sets on truthfully-trained unigram/bigram models\n",
    "T_VAL_CLEAN_T = check_for_unk_words(preprocess(T_VAL), truthful_smooth_bigram_model.tokens)\n",
    "D_VAL_CLEAN_T = check_for_unk_words(preprocess(D_VAL), truthful_smooth_bigram_model.tokens)\n",
    "\n",
    "# for computing perplexity of both validation sets on deceptively-trained unigram/bigram models\n",
    "T_VAL_CLEAN_D = check_for_unk_words(preprocess(T_VAL), deceptive_smooth_bigram_model.tokens)\n",
    "D_VAL_CLEAN_D = check_for_unk_words(preprocess(D_VAL), deceptive_smooth_bigram_model.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "575.8911106150354\n",
      "507.2203959017036\n",
      "615.8537298038397\n",
      "463.8680901825986\n",
      "1454.340989231253\n",
      "1281.9503153726475\n",
      "1324.6771814849794\n",
      "958.4681090581158\n"
     ]
    }
   ],
   "source": [
    "print(truthful_unigram_model.get_perp(T_VAL_CLEAN_T))\n",
    "print(truthful_unigram_model.get_perp(D_VAL_CLEAN_T))\n",
    "\n",
    "print(deceptive_unigram_model.get_perp(T_VAL_CLEAN_D))\n",
    "print(deceptive_unigram_model.get_perp(D_VAL_CLEAN_D))\n",
    "\n",
    "print(truthful_smooth_bigram_model.get_perp(T_VAL_CLEAN_T))\n",
    "print(truthful_smooth_bigram_model.get_perp(D_VAL_CLEAN_T))\n",
    "\n",
    "print(deceptive_smooth_bigram_model.get_perp(T_VAL_CLEAN_D))\n",
    "print(deceptive_smooth_bigram_model.get_perp(D_VAL_CLEAN_D))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_reviews(wordlist):\n",
    "    start = 0\n",
    "    reviews = []\n",
    "    for i, word in enumerate(wordlist):\n",
    "        if word == '<s>' and i != 0:\n",
    "            reviews.append(wordlist[start:i])\n",
    "            start = i+1\n",
    "    return reviews\n",
    "\n",
    "def separate_and_label_reviews(wordlist, label):\n",
    "    start = 0\n",
    "    reviews = {}\n",
    "    for i, word in enumerate(wordlist):\n",
    "        if word == '<s>' and i != 0:\n",
    "            review = tuple(wordlist[start:i])\n",
    "            reviews[review] = label\n",
    "            start = i+1\n",
    "    return reviews\n",
    "\n",
    "# for testing the accuracy of the language based classifier on the validation set\n",
    "TOKENS = check_for_unk_words(preprocess(T_VAL), truthful_smooth_bigram_model.tokens)\n",
    "TOKENS = check_for_unk_words(TOKENS, deceptive_smooth_bigram_model.tokens)\n",
    "\n",
    "VAL_REVIEWS_T = check_for_unk_words(preprocess(T_VAL), TOKENS)\n",
    "VAL_REVIEWS_D = check_for_unk_words(preprocess(D_VAL), TOKENS)\n",
    "\n",
    "VAL_REVIEWS = {**separate_and_label_reviews(VAL_REVIEWS_T, 0), **separate_and_label_reviews(VAL_REVIEWS_D, 1)}\n",
    "# now REVIEWS is a dictionary where the keys are the validation (truthful or deceptive) reviews\n",
    "# and the values are the classification labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return value of 0 ==> truthful, 1 ==> deceptive\n",
    "def classify(review, truthful_model, deceptive_model):\n",
    "    return 0 if truthful_model.get_perp(review) < deceptive_model.get_perp(review) else 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(reviews, truthful_model, deceptive_model):\n",
    "    acc = 0\n",
    "    for review in reviews:\n",
    "        if classify(review, truthful_model, deceptive_model) == reviews[review]:\n",
    "            acc += 1\n",
    "    return acc / len(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5590551181102362"
      ]
     },
     "execution_count": 465,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validate(VAL_REVIEWS, truthful_smooth_bigram_model, deceptive_smooth_bigram_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8700787401574803"
      ]
     },
     "execution_count": 466,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validate(VAL_REVIEWS, truthful_unigram_model, deceptive_unigram_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          <s>  I  like  to  run  and  hate  have  do  homework  <UNK>\n",
      "<s>         1  2     1   1    1    1     1     1   1         1      1\n",
      "I           1  1     2   1    1    1     2     1   1         1      1\n",
      "like        1  1     1   2    1    1     1     1   1         1      1\n",
      "to          1  1     1   1    2    1     1     2   2         1      1\n",
      "run         1  1     1   1    1    2     1     1   1         1      1\n",
      "and         1  2     1   1    1    1     1     1   1         1      1\n",
      "hate        1  1     1   2    1    1     1     1   1         1      1\n",
      "have        1  1     1   2    1    1     1     1   1         1      1\n",
      "do          1  1     1   1    1    1     1     1   1         2      1\n",
      "homework    1  1     1   1    1    1     1     1   1         1      1\n",
      "<UNK>       1  1     1   1    1    1     1     1   1         1      1\n",
      "7.7639360766563055\n"
     ]
    }
   ],
   "source": [
    "toy_train = preprocess('I like to run and I hate to have to do homework')\n",
    "toy_model = SmoothBigramModel(toy_train)\n",
    "print(toy_model.corpus)\n",
    "toy_val = check_for_unk_words(preprocess('I like homework'), toy_model.tokens)\n",
    "print(toy_model.get_perp(toy_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 : Kaggle Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318]\n",
      "[0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "TEST_REVIEWS = check_for_unk_words(preprocess(TEST), truthful_smooth_bigram_model.tokens)\n",
    "TEST_REVIEWS = check_for_unk_words(TEST_REVIEWS, deceptive_smooth_bigram_model.tokens)\n",
    "TEST_REVIEWS = separate_reviews(TEST_REVIEWS)\n",
    "\n",
    "def test(tmodel, dmodel, reviews):\n",
    "    ids = [i for i in range(len(reviews))]\n",
    "    preds = []\n",
    "    for review in reviews:\n",
    "        preds.append(classify(review, tmodel, dmodel))\n",
    "    return ids, preds\n",
    "\n",
    "ids, preds = test(truthful_smooth_bigram_model, deceptive_smooth_bigram_model, TEST_REVIEWS)\n",
    "print(ids)\n",
    "print(preds)\n",
    "df = pd.DataFrame({'Id': ids, 'Prediction': preds}, columns = ['Id', 'Prediction'])\n",
    "df.to_csv('Bigram-Kaggle.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
